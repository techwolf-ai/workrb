# Workflow for running model benchmark tests
# Can be triggered manually from GitHub Actions UI

name: Model Benchmarks

on:
  workflow_dispatch:
    inputs:
      test_filter:
        description: 'Pytest -k filter (e.g., "contextmatch" or leave empty for all)'
        required: false
        default: ''

jobs:
  benchmark:
    runs-on: ubuntu-latest
    name: Run Model Benchmarks

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install the latest version of uv (with Python 3.10)
        uses: astral-sh/setup-uv@v6
        with:
          python-version: "3.10"

      - name: Install the project
        run: uv sync --all-extras --dev

      - name: Run benchmark tests
        run: |
          if [ -n "${{ github.event.inputs.test_filter }}" ]; then
            uv run --no-sync pytest -m 'model_performance' -k "${{ github.event.inputs.test_filter }}" -v
          else
            uv run --no-sync poe test-benchmark
          fi

